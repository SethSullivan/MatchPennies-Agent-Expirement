{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import data_visualization as dv\n",
    "wheel = dv.ColorWheel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_decision_time_v2(agent_mean, agent_std, reaction_time,movement_time, reaction_uncertainty, movement_uncertainty,timing_uncertainty,plot=False):\n",
    "    timesteps = np.arange(0,2000,1)\n",
    "    prob_agent_go = np.zeros(len(timesteps))\n",
    "    prob_agent_no_go = np.zeros(len(timesteps))\n",
    "    prob_making_react = np.zeros(len(timesteps))\n",
    "    prob_making_gamble = np.zeros(len(timesteps))\n",
    "    prob_agent_making = np.zeros(len(timesteps))\n",
    "\n",
    "    expected_reward = np.zeros(len(timesteps))\n",
    "    expected_reward_gamble = np.zeros(len(timesteps))\n",
    "    expected_reward_react = np.zeros(len(timesteps))\n",
    "    prob_reward_gamble = np.zeros(len(timesteps))\n",
    "    prob_reward_react = np.zeros(len(timesteps))\n",
    "    prob_reward = np.zeros(len(timesteps))\n",
    "    reaction_movement_time = reaction_time + movement_time\n",
    "\n",
    "\n",
    "    prob_success_gamble = 0.5 # IT'S NOT ABOUT THE PROBABILITY THAT I WILL SUCCEED WITH EACH ACTION... IT'S ABOUT THE PROBABILITY THAT I WILL BE ABLE TO SELECT THE OPTIMAL ACTION\n",
    "    prob_success_react = 1.0\n",
    "    win_reward = 1.0\n",
    "    loss_cost = 0\n",
    "    indecision_cost = 0\n",
    "    \n",
    "    prob_agent_go = stats.norm.cdf(timesteps,agent_mean,agent_std)\n",
    "    prob_agent_no_go = 1 - prob_agent_go\n",
    "    prob_agent_making = stats.norm.cdf(1500-timesteps,agent_mean,agent_std)\n",
    "    \n",
    "    \n",
    "    # Calculate reaction movement uncertainty basedon prob_agent_go\n",
    "    reaction_movement_uncertainty = np.sqrt((movement_uncertainty)**2 + prob_agent_go*(reaction_uncertainty**2)) # We have some uncertainty about making it  \n",
    "    total_uncertainty_reaction = np.sqrt(reaction_movement_uncertainty**2) #+ timing_uncertainty**2)\n",
    "    total_uncertainty_gamble = np.sqrt(movement_uncertainty**2 + timing_uncertainty**2)  \n",
    "    \n",
    "    prob_making_react = stats.norm.cdf(1500-timesteps,reaction_movement_time,total_uncertainty_reaction)\n",
    "    prob_making_gamble = stats.norm.cdf(1500-timesteps,movement_time,total_uncertainty_gamble)\n",
    "\n",
    "    # THESE AREN'T Multiplied by Agent Probability     \n",
    "    prob_reward_gamble = (prob_success_gamble)*prob_making_gamble \n",
    "    prob_cost_gamble = (1-prob_success_gamble)*prob_making_gamble # Probability of receiving the Cost for being incorrect\n",
    "    \n",
    "    prob_reward_react = (prob_success_react)*prob_making_react \n",
    "    prob_cost_react = ((1-prob_success_react))*prob_making_react # Probability of receiving the Cost for being incorrect\n",
    "    \n",
    "    prob_indecision_gamble = (1 - prob_making_gamble)\n",
    "    prob_indecision_react = (1 - prob_making_react)    \n",
    "    \n",
    "    # Don't need AND subtraction because the prob_agent_go and prob_agent_no_go takes care of it\n",
    "    # These are multiplied by agent probability, because that's the probability that I SELECT GAMBLE and SELECT REACTION\n",
    "    prob_reward = prob_reward_gamble*prob_agent_no_go + prob_reward_react*prob_agent_go #- (prob_reward_gamble*prob_reward_react) # NEED TO SUBTRACT THE AND PROBABILITY?... this makes sense... If I go at that time, I at least have 0.5 PLUS the probability that the agent has gone will boost it up\n",
    "    prob_cost = prob_cost_gamble*prob_agent_no_go + prob_cost_react*prob_agent_go # - (prob_cost_react*prob_cost_gamble)\n",
    "    prob_indecision = prob_indecision_gamble*prob_agent_no_go + prob_indecision_react*prob_agent_go #- (prob_indecision_gamble*prob_indecision_react) # Probability of indecision depends on if someone gambles\n",
    "    \n",
    "    #prob_indecision = prob_indecision_gamble*(1-B) + prob_indecision_react*B - prob_indecision_gamble*(1-B)*B*prob_indecision_react # Probability of indecision depends on if someone gambles \n",
    "    \n",
    "    exp_reward_gamble = prob_reward_gamble*win_reward + prob_cost_gamble*loss_cost + prob_indecision_gamble*indecision_cost\n",
    "    exp_reward_react = prob_reward_react*win_reward + prob_cost_react*loss_cost + prob_indecision_gamble*indecision_cost\n",
    "    exp_reward = prob_reward*win_reward + prob_cost*loss_cost + (prob_indecision)*indecision_cost\n",
    "    \n",
    "    return np.argmax(exp_reward),np.max(exp_reward),total_uncertainty_reaction,total_uncertainty_gamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_optimals(timesteps,exp_reward,exp_reward_gamble,exp_reward_react,prob_reward_gamble,prob_reward_react,\n",
    "#                   prob_agent_go,prob_agent_no_go,\n",
    "#                   prob_reward, movement_time,reaction_movement_time):\n",
    "#     fig,ax = plt.subplots(dpi = 125)\n",
    "#     ax.plot(timesteps, exp_reward, label = 'Exp Reward', c = wheel.green, ls = '--')\n",
    "#     ax.plot(timesteps, exp_reward_gamble, label = 'Exp Reward Gamble')\n",
    "#     ax.plot(timesteps, exp_reward_react, label = 'Exp Reward React')\n",
    "#     ax.plot(timesteps,prob_reward_gamble*prob_agent_no_go,label='Gamble Reward*ProbAgentNoGo')\n",
    "#     ax.plot(timesteps,prob_reward_react*prob_agent_go,label = 'React Reward*ProbAgentGo')\n",
    "#     # ax.plot(timesteps, prob_indecision_gamble)\n",
    "#     ax.set_ylim(-1,1.1)\n",
    "#     ax.set_xlim(0,1500)\n",
    "#     ax.set_xticks(np.arange(0,2000,300))\n",
    "#     ax.set_xlabel('Time (ms)')\n",
    "#     ax.set_ylabel('Expected Reward')\n",
    "#     ax.legend(fontsize = 8,loc = (0.01,0.1))\n",
    "#     ax.set_title(f'Gain Function for Decision Time')#\\n B = {B}')\n",
    "#     plt.show()\n",
    "#     optimal_decision_time  = np.argmax(prob_reward) \n",
    "#     target_reach_time_on_gambles = optimal_decision_time + movement_time \n",
    "#     target_reach_time_on_reactions = optimal_decision_time + reaction_movement_time\n",
    "#     print(f'Optimal Decision Time: {optimal_decision_time}') # Remember the task decision time is this \n",
    "#     print(f'Maximum Expected Reward: {np.max(prob_reward)}')\n",
    "#     print(f'Gamble Target Reach Time: {target_reach_time_on_gambles}')\n",
    "#     print(f'Reaction Target Reach Time: {target_reach_time_on_reactions}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 0.1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "mu = 1000\n",
    "std = 100\n",
    "clip_a,clib_b = 1000,1033\n",
    "a,b = (clip_a - mu)/std, (clib_b - mu)/std\n",
    "x = np.arange(0,2000,1)\n",
    "trunc_pdf = stats.truncnorm.pdf(x,a,b)\n",
    "pdf = stats.norm.pdf(x,mu,std)\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(x,pdf)\n",
    "ax.plot(x,trunc_pdf)\n",
    "rv = stats.truncnorm.pdf(x,10,20,loc=mu,scale=std)\n",
    "ax.plot(x,rv)\n",
    "ax.set_ylim(0,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a00c1525417994d84940f6a64b96d4df953e4f0863c4f32c2c802abdf8195ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
