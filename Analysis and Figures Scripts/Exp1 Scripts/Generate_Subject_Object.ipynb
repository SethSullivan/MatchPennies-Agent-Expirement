{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Thangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\\\Subject_Data\\\\Seth_MatchPennies_Agent_Exp1\\\\')\n",
    "PATH = os.getcwd()\n",
    "SAVE_PATH = 'C:\\\\Users\\\\Seth Sullivan\\\\OneDrive - University of Delaware - o365\\\\Desktop\\\\MatchPennies-Agent-Expirement\\\\Modeling Figures\\\\Optimal Stopping Expected Reward Figures Group\\\\'\n",
    "if not os.path.exists(SAVE_PATH):\n",
    "    os.makedirs(SAVE_PATH)\n",
    "\n",
    "# Fields pull and pull list\n",
    "os.chdir('D:\\Subject_Data\\Seth_MatchPennies_Agent_Exp1')\n",
    "PATH = os.getcwd()\n",
    "figures_pull_list = []\n",
    "figures_pull_list_control = []\n",
    "figures_pull_list_task = []\n",
    "fields_pull = []\n",
    "with open(PATH+\"\\\\Figures_Pull_List.txt\", \"r\") as pull_file:\n",
    "    figures_pull_list = pull_file.read().splitlines()\n",
    "with open(PATH+\"\\\\Figures_Pull_List_Control.txt\", \"r\") as pull_file:\n",
    "    analysis_pull_list_control = pull_file.read().splitlines()\n",
    "with open(PATH+\"\\\\Figures_Pull_List_Task.txt\", \"r\") as pull_file:\n",
    "    analysis_pull_list_task = pull_file.read().splitlines()\n",
    "with open(PATH+\"\\\\Fields_Pull.txt\", \"r\") as fields_pull:\n",
    "    fields_pull = fields_pull.read().splitlines()\n",
    "NUM_SUBJECTS = len(figures_pull_list)\n",
    "task_name = 'Seth_MatchPennies_Agent_Exp1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle Load Control Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "coincidence_trials = 50\n",
    "reaction_trials = 50\n",
    "interval_trials = 50\n",
    "# ---------------Controls-------------------------\n",
    "player_reaction_decision_array = np.empty((NUM_SUBJECTS, reaction_trials))\n",
    "agent_reaction_decision_array = np.empty((NUM_SUBJECTS, reaction_trials))\n",
    "player_reaction_time = np.zeros([NUM_SUBJECTS,reaction_trials])*np.nan \n",
    "player_reaction_movement_time = np.zeros([NUM_SUBJECTS,reaction_trials])*np.nan\n",
    "reaction_trial_start = np.zeros((NUM_SUBJECTS,reaction_trials))*np.nan \n",
    "coincidence_trial_start = np.zeros((NUM_SUBJECTS, coincidence_trials))*np.nan\n",
    "coincidence_reach_time = np.zeros((NUM_SUBJECTS, coincidence_trials))*np.nan\n",
    "interval_trial_start = np.zeros((NUM_SUBJECTS, interval_trials))*np.nan\n",
    "interval_reach_time = np.zeros((NUM_SUBJECTS, interval_trials))*np.nan\n",
    "\n",
    "for i in range(NUM_SUBJECTS):\n",
    "    subname = figures_pull_list[i]\n",
    "    data_path = PATH+f'\\\\Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    reaction_trial_start[i,:] = pickle.load(open(data_path + f'{subname}_reaction_trial_start.pkl', 'rb'))\n",
    "    player_reaction_time[i,:] = pickle.load(open(data_path + f'{subname}_player_reaction_time.pkl', 'rb'))\n",
    "    player_reaction_movement_time[i,:] = pickle.load(open(data_path + f'{subname}_player_reaction_movement_time.pkl', 'rb'))\n",
    "    player_reaction_decision_array[i,:] = pickle.load(open(data_path + f'{subname}_player_reaction_decision_array.pkl', 'rb'))\n",
    "    agent_reaction_decision_array[i,:] = pickle.load(open(data_path + f'{subname}_agent_reaction_decision_time.pkl', 'rb'))\n",
    "    reaction_trial_start[i,:] = pickle.load(open(data_path + f'{subname}_reaction_trial_start.pkl', 'rb'))\n",
    "    interval_trial_start[i,:] = pickle.load(open(data_path + f'{subname}_interval_trial_start.pkl', 'rb'))\n",
    "    interval_reach_time[i,:] = pickle.load(open(data_path + f'{subname}_interval_reach_time.pkl', 'rb'))\n",
    "    coincidence_trial_start[i,:] = pickle.load(open(data_path + f'{subname}_coincidence_trial_start.pkl', 'rb'))\n",
    "    coincidence_reach_time[i,:] =  pickle.load(open(data_path + f'{subname}_coincidence_reach_time.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickle Load Task and Washout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = PATH+'\\\\'+'Sub1_Task'\n",
    "task_df = pd.read_csv(path1+f'\\\\Sub1_TaskTrial_Table.csv')\n",
    "task_df = task_df.loc[task_df['Condition type']==3] # Only get the task condition \n",
    "num_trials = int(task_df.iloc[-1]['Block_Step']) # number of trials in each block\n",
    "num_blocks = int(task_df.iloc[-1]['Block_Row']/2)\n",
    "tot_trials = int(num_trials*num_blocks)\n",
    "trial_time = int(task_df.iloc[0]['Condition time'])\n",
    "task_df_columns = len(fields_pull)\n",
    "trial_table = np.empty((NUM_SUBJECTS, tot_trials, 4), int)\n",
    "#----------------- TASK--------------------------------\n",
    "player_task_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "player_task_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "player_task_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "player_task_movement_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "agent_task_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "agent_task_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "agent_task_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "\n",
    "# -------------- Washout-------------------\n",
    "washout_trials = 25\n",
    "player_washout_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "player_washout_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "player_washout_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "player_washout_movement_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "agent_washout_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "agent_washout_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "agent_washout_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "agent_washout_movement_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "\n",
    "for i in range(NUM_SUBJECTS):\n",
    "    subname = figures_pull_list[i]\n",
    "    data_path = PATH+f'\\\\Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    #task_data[i,:,:,:,:] = pickle.load(open(data_path + f'{subname}_task_data.pkl', 'rb'))\n",
    "    player_task_decision_time[i,:,:] = pickle.load( open(data_path + f'{subname}_player_task_decision_time.pkl', 'rb'))\n",
    "    player_task_decision_array[i,:,:] = pickle.load( open(data_path + f'{subname}_player_task_decision_array.pkl', 'rb'))\n",
    "    player_task_movement_time[i,:,:] = pickle.load(open(data_path + f'{subname}_player_task_movement_time.pkl', 'rb'))\n",
    "    player_task_reach_time[i,:,:] = pickle.load( open(data_path + f'{subname}_player_task_reach_time.pkl', 'rb'))\n",
    "    agent_task_decision_time[i,:,:] = pickle.load( open(data_path + f'{subname}_agent_task_decision_time.pkl', 'rb'))\n",
    "    agent_task_decision_array[i,:,:] = pickle.load(open(data_path + f'{subname}_agent_task_decision_array.pkl', 'rb'))\n",
    "    agent_task_reach_time[i,:,:] = agent_task_decision_time[i,:,:] + 150\n",
    "    player_washout_decision_time[i,:,:] = pickle.load( open(data_path + f'{subname}_player_washout_decision_time.pkl', 'rb'))\n",
    "    player_washout_decision_array[i,:,:] = pickle.load( open(data_path + f'{subname}_player_washout_decision_array.pkl', 'rb'))\n",
    "    player_washout_movement_time[i,:,:] = pickle.load(open(data_path + f'{subname}_player_washout_movement_time.pkl', 'rb'))\n",
    "    player_washout_reach_time[i,:,:] = pickle.load( open(data_path + f'{subname}_player_washout_reach_time.pkl', 'rb'))\n",
    "    agent_washout_decision_time[i,:,:] = pickle.load( open(data_path + f'{subname}_agent_washout_decision_time.pkl', 'rb'))\n",
    "    agent_washout_decision_array[i,:,:] = pickle.load(open(data_path + f'{subname}_agent_washout_decision_array.pkl', 'rb'))\n",
    "    agent_washout_reach_time[i,:,:] = pickle.load(open(data_path + f'{subname}_agent_washout_reach_time.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subject():\n",
    "    def __init__(self, **kwargs):\n",
    "        self.num_blocks = kwargs.get('num_blocks',1)\n",
    "        self.num_trials = kwargs.get('num_trials',1)\n",
    "        self.num_control_trials = kwargs.get('num_control_trials',1)\n",
    "        self.num_washout_trials = kwargs.get('num_washout_trials',1)                   \n",
    "        # Control data\n",
    "        self.reaction_time =           kwargs.get('reaction_time', np.zeros((self.num_control_trials)))\n",
    "        self.reaction_movement_time =  kwargs.get('reaction_movement_time', np.zeros(self.num_control_trials))\n",
    "        self.interval_trial_start =    kwargs.get('interval_trial_start', np.zeros((self.num_control_trials)))\n",
    "        self.interval_reach_time =     kwargs.get('interval_reach_time', np.zeros((self.num_control_trials)))\n",
    "        self.coincidence_trial_start = kwargs.get('coincidence_trial_start', np.zeros((self.num_control_trials)))\n",
    "        self.coincidence_reach_time =  kwargs.get('coincidence_reach_time', np.zeros((self.num_control_trials)))\n",
    "        # Washout data\n",
    "        self.player_washout_decision_time =  kwargs.get('player_washout_decision_time',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.player_washout_decision_array = kwargs.get('player_washout_decision_array',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.player_washout_movement_time =  kwargs.get('player_washout_movement_time',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.player_washout_reach_time =     kwargs.get('player_washout_reach_time',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.agent_washout_decision_time =   kwargs.get('agent_washout_decision_time',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.agent_washout_decision_array =  kwargs.get('agent_washout_decision_array',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.agent_washout_movement_time =   kwargs.get('agent_washout_movement_time',np.zeros((self.num_blocks,self.num_washout_trials)))\n",
    "        self.agent_washout_reach_time =      kwargs.get('agent_washout_reach_time',np.zeros((self.num_blocks,self.num_washout_trials)))       \n",
    "        # Task data\n",
    "        self.player_task_decision_time =  kwargs.get('player_task_decision_time',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.player_task_decision_array = kwargs.get('player_task_decision_array',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.player_task_movement_time =  kwargs.get('player_task_movement_time',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.player_task_reach_time =     kwargs.get('player_task_reach_time',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.agent_task_decision_time =   kwargs.get('agent_task_decision_time',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.agent_task_decision_array =  kwargs.get('agent_task_decision_array',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.agent_task_movement_time =   kwargs.get('agent_task_movement_time',np.zeros((self.num_blocks,self.num_trials)))\n",
    "        self.agent_task_reach_time =      kwargs.get('agent_task_reach_time',np.zeros((self.num_blocks,self.num_trials))) \n",
    "        self.player_minus_agent_task_decision_time = self.player_task_decision_time - self.agent_task_decision_time\n",
    "        \n",
    "        self.num_stds_for_reaction_time = kwargs.get('num_stds_for_reaction_time',2)\n",
    "        self.n = kwargs.get('cutoff_for_controls_calc',10)\n",
    "        \n",
    "        #------------------------Calculate Means and Stds----------------------------------------------------------------------------------------------------------\n",
    "        # Control means\n",
    "        self.reaction_time_mean = np.nanmean(self.reaction_time[self.n:])\n",
    "        self.reaction_movement_time_mean = np.nanmean(self.reaction_movement_time[self.n:])\n",
    "        self.reaction_plus_movement_time_mean = np.nanmean(self.reaction_time[self.n:] + self.reaction_movement_time[self.n:])\n",
    "        self.interval_reach_time_mean = np.nanmean(self.interval_reach_time[self.n:])\n",
    "        self.coincidence_reach_time_mean = np.nanmean(self.coincidence_reach_time[self.n:])\n",
    "        # Control stds\n",
    "        self.reaction_time_sd = np.nanstd(self.reaction_time[self.n:])\n",
    "        self.reaction_movement_time_sd = np.nanstd(self.reaction_movement_time[self.n:])\n",
    "        self.reaction_plus_movement_time_sd = np.nanstd(self.reaction_time[self.n:] + self.reaction_movement_time[self.n:])\n",
    "        self.interval_reach_time_sd = np.nanstd(self.interval_reach_time[self.n:])\n",
    "        self.coincidence_reach_time_sd = np.nanstd(self.coincidence_reach_time[self.n:])\n",
    "        # Task means and stds\n",
    "        self.agent_mean_task_reach_time = np.nanmean(self.agent_task_reach_time,axis = 1)\n",
    "        self.agent_median_reach_time = np.nanmedian(self.agent_task_reach_time,axis = 1)\n",
    "        self.agent_sd_task_reach_time = np.nanstd(self.agent_task_reach_time,axis = 1)\n",
    "        self.agent_mean_task_decision_time = np.nanmean(self.agent_task_decision_time, axis = 1)\n",
    "        self.agent_median_task_decision_time = np.nanmedian(self.agent_task_decision_time, axis = 1)\n",
    "        self.agent_sd_task_decision_time = np.nanstd(self.agent_task_decision_time, axis = 1)\n",
    "        self.player_mean_task_reach_time = np.nanmean(self.player_task_reach_time,axis = 1)\n",
    "        self.player_median_reach_time = np.nanmedian(self.player_task_reach_time,axis = 1)\n",
    "        self.player_sd_task_reach_time = np.nanstd(self.player_task_reach_time,axis = 1)\n",
    "        self.player_mean_task_decision_time = np.nanmean(self.player_task_decision_time,axis = 1)\n",
    "        self.player_median_task_decision_time = np.nanmedian(self.player_task_decision_time,axis = 1)\n",
    "        self.player_sd_task_decision_time = np.nanstd(self.agent_task_reach_time,axis = 1)\n",
    "        self.player_mean_task_movement_time = np.nanmean(self.player_mean_task_reach_time - self.player_mean_task_decision_time)\n",
    "        \n",
    "        self.player_minus_agent_mean_task_decision_time = np.nanmean(self.player_minus_agent_task_decision_time, axis = 1)\n",
    "        \n",
    "        #------------------------------------------------------------------------------------------------------------------\n",
    "        # Task Indecisions, Wins, Incorrects\n",
    "        self.player_wins,self.player_indecisions,self.player_incorrects = self.calc_wins_indecisions_incorrects()\n",
    "        self.perc_wins = (self.player_wins/self.num_trials)*100\n",
    "        self.perc_indecisions = (self.player_indecisions/self.num_trials)*100\n",
    "        self.perc_incorrects = (self.player_incorrects/self.num_trials)*100\n",
    "        \n",
    "        # Reach Times on Indecisions\n",
    "        self.agent_task_decision_time_on_indecisions,self.player_task_reach_time_on_indecisions,\\\n",
    "            self.player_task_decision_time_on_indecisions,self.agent_mean_task_reach_time_on_indecisions,\\\n",
    "            self.player_mean_task_reach_time_on_indecisions,self.player_mean_task_decision_time_on_indecisions = self.decision_and_reach_times_on_indecisions()\n",
    "                \n",
    "        # Gamble and Reaction Calculations\n",
    "        self.perc_reactions,self.perc_reaction_wins,self.perc_reaction_incorrects,self.perc_reaction_indecisions,\\\n",
    "            self.perc_wins_that_were_reactions,self.perc_indecisions_that_were_reactions,self.perc_incorrects_that_were_reactions,\\\n",
    "            self.reaction_decision_time_means, self.perc_gambles,self.perc_gamble_wins,self.perc_gamble_incorrects,self.perc_gamble_indecisions,\\\n",
    "            self.perc_wins_that_were_gambles,self.perc_indecisions_that_were_gambles,self.perc_incorrects_that_were_gambles, self.gamble_decision_time_means = self.reaction_gamble_calculations(self.num_stds_for_reaction_time)\n",
    "        \n",
    "        self.player_perc_wins_when_both_decide,self.player_wins_when_both_decide = self.wins_when_both_decide()\n",
    "    def calc_wins_indecisions_incorrects(self):\n",
    "        player_indecisions = np.zeros((num_blocks))\n",
    "        player_wins = np.zeros((num_blocks))\n",
    "        player_incorrects = np.zeros((num_blocks))\n",
    "        for i in range(num_blocks):\n",
    "            player_indecisions[i] = np.count_nonzero(self.player_task_decision_array[i,:] == 0)\n",
    "            player_wins[i] = np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:] == 1 , self.agent_task_decision_array[i,:] == 1))\n",
    "            player_wins[i]+= np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:] == -1 , self.agent_task_decision_array[i,:] == -1))\n",
    "            player_wins[i]+= np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:] == -1 , self.agent_task_decision_array[i,:] == 0))\n",
    "            player_wins[i]+= np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:] == 1 , self.agent_task_decision_array[i,:] == 0))\n",
    "            player_incorrects[i] = np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:] == 1 , self.agent_task_decision_array[i,:] == -1))\n",
    "            player_incorrects[i] += np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:] == -1 , self.agent_task_decision_array[i,:] == 1))\n",
    "        return player_wins,player_indecisions,player_incorrects\n",
    "    \n",
    "    def decision_and_reach_times_on_indecisions(self):\n",
    "        agent_task_decision_time_on_indecisions = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        player_task_reach_time_on_indecisions = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        player_left_time_on_indecisions = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        indecision_index = np.argwhere(self.player_task_reach_time>1500)\n",
    "\n",
    "        c=0\n",
    "        for j,k in indecision_index:\n",
    "            agent_task_decision_time_on_indecisions[j,k] = self.agent_task_decision_time[j,k]\n",
    "            player_task_reach_time_on_indecisions[j,k] = self.player_task_reach_time[j,k]\n",
    "            player_left_time_on_indecisions[j,k] = self.player_task_decision_time[j,k]\n",
    "            c+=1\n",
    "\n",
    "        agent_mean_task_reach_time_on_indecisions = np.nanmean(agent_task_decision_time_on_indecisions,axis=1)\n",
    "        player_mean_task_reach_time_on_indecisions = np.nanmean(player_task_reach_time_on_indecisions,axis=1)\n",
    "        player_mean_left_time_on_indecisions = np.nanmean(player_left_time_on_indecisions,axis=1)\n",
    "        return agent_task_decision_time_on_indecisions,player_task_reach_time_on_indecisions,player_left_time_on_indecisions,\\\n",
    "            agent_mean_task_reach_time_on_indecisions,player_mean_task_reach_time_on_indecisions,player_mean_left_time_on_indecisions\n",
    "            \n",
    "    def reaction_gamble_calculations(self,num_stds):\n",
    "        # Gamble arrays\n",
    "        gamble_decision_time = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        gamble_reach_target_time = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        agent_task_reach_time_gambles = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        agent_task_decision_time_gambles = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        # Reaction arrays\n",
    "        reaction_decision_time = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        reaction_reach_target_time = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        agent_task_reach_time_reactions = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "        agent_task_decision_time_reactions = np.zeros((self.num_blocks,self.num_trials))*np.nan\n",
    "\n",
    "        # Wins, indecisiosn, incorrects arrays\n",
    "        gamble_wins = np.zeros((self.num_blocks))\n",
    "        perc_gamble_wins = np.zeros((self.num_blocks))\n",
    "        gamble_indecisions = np.zeros((self.num_blocks))\n",
    "        perc_gamble_indecisions = np.zeros((self.num_blocks))\n",
    "        gamble_incorrects = np.zeros((self.num_blocks))\n",
    "        perc_gamble_incorrects = np.zeros((self.num_blocks))\n",
    "        reaction_wins = np.zeros((self.num_blocks))\n",
    "        perc_reaction_wins = np.zeros((self.num_blocks))\n",
    "        reaction_indecisions = np.zeros((self.num_blocks))\n",
    "        perc_reaction_indecisions = np.zeros((self.num_blocks))\n",
    "        reaction_incorrects = np.zeros((self.num_blocks))\n",
    "        perc_reaction_incorrects = np.zeros((self.num_blocks))\n",
    "        total_gambles = np.zeros((self.num_blocks))\n",
    "        total_reactions = np.zeros((self.num_blocks))\n",
    "        total_did_not_leave = np.zeros((self.num_blocks))\n",
    "\n",
    "        temp_player_reaction_time =  self.reaction_time_mean - num_stds*self.reaction_time_sd\n",
    "        gamble_index = np.argwhere((self.player_task_decision_time-self.agent_task_decision_time)<=temp_player_reaction_time)\n",
    "        reaction_index = np.argwhere((self.player_task_decision_time-self.agent_task_decision_time)>temp_player_reaction_time)\n",
    "        did_not_leave_start_index = np.argwhere(np.isnan(self.player_task_decision_time))\n",
    "        for i,j in gamble_index:\n",
    "            gamble_decision_time[i,j] = self.player_task_decision_time[i,j]\n",
    "            gamble_reach_target_time[i,j] = self.player_task_reach_time[i,j]\n",
    "            agent_task_reach_time_gambles[i,j] = self.agent_task_reach_time[i,j]\n",
    "            agent_task_decision_time_gambles[i,j] = self.agent_task_decision_time[i,j]\n",
    "            # Calculate gamble wins\n",
    "            if self.player_task_decision_array[i,j] == 1 and (self.agent_task_decision_array[i,j] == 1 or self.agent_task_decision_array[i,j] == 0):\n",
    "                gamble_wins[i] += 1\n",
    "            elif self.player_task_decision_array[i,j] == -1 and (self.agent_task_decision_array[i,j] == -1 or self.agent_task_decision_array[i,j] == 0):\n",
    "                gamble_wins[i] += 1\n",
    "            elif self.player_task_decision_array[i,j] == 0:\n",
    "                gamble_indecisions[i] += 1\n",
    "            elif self.player_task_decision_array[i,j]*self.agent_task_decision_array[i,j] == -1:\n",
    "                gamble_incorrects[i] += 1\n",
    "            else:\n",
    "                print('none')\n",
    "            total_gambles[i]+=1\n",
    "        for i,j in reaction_index:\n",
    "            reaction_decision_time[i,j] = self.player_task_decision_time[i,j]\n",
    "            reaction_reach_target_time[i,j] = self.player_task_reach_time[i,j]\n",
    "            agent_task_reach_time_reactions[i,j] = self.agent_task_reach_time[i,j]\n",
    "            agent_task_decision_time_reactions[i,j] = self.agent_task_decision_time[i,j]\n",
    "            # Calculate reaction wins\n",
    "            if self.player_task_decision_array[i,j] == 1 and (self.agent_task_decision_array[i,j] == 1 or self.agent_task_decision_array[i,j] == 0):\n",
    "                reaction_wins[i] += 1\n",
    "            elif self.player_task_decision_array[i,j] == -1 and (self.agent_task_decision_array[i,j] == -1 or self.agent_task_decision_array[i,j] == 0):\n",
    "                reaction_wins[i] += 1\n",
    "            elif self.player_task_decision_array[i,j] == 0:\n",
    "                reaction_indecisions[i] += 1\n",
    "            elif self.player_task_decision_array[i,j]*self.agent_task_decision_array[i,j] == -1:\n",
    "                reaction_incorrects[i] += 1\n",
    "            else:\n",
    "                print('none')\n",
    "            total_reactions[i]+=1\n",
    "                \n",
    "        for i,j in did_not_leave_start_index:\n",
    "            reaction_indecisions[i]+=1\n",
    "            total_did_not_leave[i]+=1\n",
    "        perc_reactions = total_reactions/self.num_trials*100\n",
    "        perc_reaction_wins = reaction_wins/total_reactions*100 # Array division\n",
    "        perc_reaction_incorrects = reaction_incorrects/total_reactions*100\n",
    "        perc_reaction_indecisions = reaction_indecisions/total_reactions*100\n",
    "\n",
    "        perc_gambles = total_gambles/self.num_trials*100\n",
    "        perc_gamble_wins = gamble_wins/total_gambles*100\n",
    "        perc_gamble_incorrects = gamble_incorrects/total_gambles*100\n",
    "        perc_gamble_indecisions = gamble_indecisions/total_gambles*100\n",
    "\n",
    "        perc_wins_that_were_gambles = gamble_wins/self.player_wins *100\n",
    "        perc_indecisions_that_were_gambles = gamble_indecisions/self.player_indecisions*100\n",
    "        perc_incorrects_that_were_gambles = gamble_indecisions/self.player_incorrects*100\n",
    "\n",
    "        perc_wins_that_were_reactions = reaction_wins/self.player_wins*100\n",
    "        perc_indecisions_that_were_reactions = reaction_indecisions/self.player_indecisions*100\n",
    "        perc_incorrects_that_were_reactions = reaction_indecisions/self.player_incorrects*100\n",
    "\n",
    "\n",
    "        # get means\n",
    "        gamble_decision_time_means = np.nanmean(gamble_decision_time, axis = 1 )\n",
    "        reaction_decision_time_means = np.nanmean(reaction_decision_time, axis = 1 )\n",
    "        agent_task_decision_time_gamble_means = np.nanmean(agent_task_decision_time_gambles, axis = 1)\n",
    "        agent_task_decision_time_reaction_means = np.nanmean(agent_task_decision_time_reactions, axis = 1)\n",
    "\n",
    "        # Not sure why I had this threshold in??\n",
    "        # for i in range(self.num_blocks):\n",
    "        #     if total_reactions[i]<10:\n",
    "        #         perc_reaction_wins[i] = np.nan\n",
    "        #         perc_reaction_incorrects[i] = np.nan\n",
    "        #         perc_reaction_indecisions[i] = np.nan\n",
    "        #         reaction_decision_time_means[i] = np.nan\n",
    "\n",
    "        #     if total_gambles[i]<10:\n",
    "        #         perc_gamble_wins[i] = np.nan\n",
    "        #         perc_gamble_incorrects[i] = np.nan\n",
    "        #         perc_gamble_indecisions[i] = np.nan\n",
    "        #         gamble_decision_time_means[i] = np.nan\n",
    "        \n",
    "        return perc_reactions,perc_reaction_wins,perc_reaction_incorrects,perc_reaction_indecisions,\\\n",
    "            perc_wins_that_were_reactions,perc_indecisions_that_were_reactions,perc_incorrects_that_were_reactions, reaction_decision_time_means,\\\n",
    "            perc_gambles,perc_gamble_wins,perc_gamble_incorrects,perc_gamble_indecisions,\\\n",
    "            perc_wins_that_were_gambles,perc_indecisions_that_were_gambles,perc_incorrects_that_were_gambles, gamble_decision_time_means\n",
    "   \n",
    "    def wins_when_both_decide(self):\n",
    "        # Get agent decision array\n",
    "        player_both_reached_wins = np.zeros((self.num_blocks))\n",
    "        perc_player_both_reached_wins = np.zeros((self.num_blocks))\n",
    "        agent_both_reached_wins = np.zeros((self.num_blocks))\n",
    "        for i in range(self.num_blocks):\n",
    "            for j in range(self.num_trials):\n",
    "                if self.agent_task_reach_time[i,j]>1500:\n",
    "                    self.agent_task_decision_array[i,j] = 0\n",
    "        # Get wins when both decide\n",
    "        for i in range(self.num_blocks):\n",
    "            for j in range(self.num_trials):\n",
    "                if self.agent_task_decision_array[i,j]*self.player_task_decision_array[i,j] == 1:\n",
    "                    player_both_reached_wins[i]+=1\n",
    "                if self.agent_task_decision_array[i,j]*self.player_task_decision_array[i,j] == -1:\n",
    "                    agent_both_reached_wins[i]+=1\n",
    "            x = np.count_nonzero(self.player_task_decision_array[i,:]!=0)\n",
    "            y = np.count_nonzero(self.agent_task_decision_array[i,:]!=0)\n",
    "            if x!= 0 and y!= 0:\n",
    "                total = np.count_nonzero(np.logical_and(self.player_task_decision_array[i,:]!=0,self.agent_task_decision_array[i,:]!=0))\n",
    "                perc_player_both_reached_wins[i] = (player_both_reached_wins[i]/total)*100\n",
    "        return perc_player_both_reached_wins,player_both_reached_wins\n",
    "        \n",
    "    \n",
    "            \n",
    "    \n",
    "            \n",
    "        \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Group():\n",
    "    def __init__(self,objects):\n",
    "        self.objects = objects\n",
    "        \n",
    "        # Control tasks, group means\n",
    "        self.reaction_time_mean = np.nanmean(self.list_of_metrics('reaction_time_mean'))\n",
    "        self.reaction_movement_time_mean = np.nanmean(self.list_of_metrics('reaction_movement_time_mean'))\n",
    "        self.reaction_movement_time_mean = np.nanmean(self.list_of_metrics('reaction_movement_time_mean'))\n",
    "        self.reaction_plus_movement_time_mean = np.nanmean(self.list_of_metrics('reaction_plus_movement_time_mean'))\n",
    "        self.interval_reach_time_mean = np.nanmean(self.list_of_metrics('interval_reach_time_mean'))\n",
    "        self.coincidence_reach_time_mean = np.nanmean(self.list_of_metrics('coincidence_reach_time_mean'))\n",
    "        \n",
    "        # Control tasks, group uncertainty\n",
    "        self.reaction_time_sd = np.nanmean(self.list_of_metrics('reaction_time_sd'))\n",
    "        self.reaction_movement_time_sd = np.nanmean(self.list_of_metrics('reaction_movement_time_sd'))\n",
    "        self.reaction_plus_movement_time_sd = np.nanmean(self.list_of_metrics('reaction_plus_movement_time_sd'))\n",
    "        self.interval_reach_time_sd = np.nanmean(self.list_of_metrics('interval_reach_time_sd'))\n",
    "        self.coincidence_reach_time_sd = np.nanmean(self.list_of_metrics('coincidence_reach_time_sd'))\n",
    "        \n",
    "        # Task means and stds\n",
    "        self.agent_mean_task_reach_time = np.nanmean(self.list_of_metrics('agent_mean_task_reach_time'),axis = 0)\n",
    "        self.agent_median_reach_time = np.nanmedian(self.list_of_metrics('agent_mean_task_reach_time'),axis = 0)\n",
    "        self.agent_sd_task_reach_time = np.nanstd(self.list_of_metrics('agent_mean_task_reach_time'),axis = 0)\n",
    "        self.agent_mean_task_decision_time = np.nanmean(self.list_of_metrics('agent_mean_task_decision_time'),axis =0)\n",
    "        self.agent_median_task_decision_time = np.nanmedian(self.list_of_metrics('agent_mean_task_decision_time'),axis =0)\n",
    "        self.agent_sd_task_decision_time = np.nanstd(self.list_of_metrics('agent_mean_task_decision_time'),axis =0)\n",
    "        self.player_mean_task_reach_time = np.nanmean(self.list_of_metrics('player_mean_task_reach_time'),axis=0)\n",
    "        self.player_median_reach_time = np.nanmedian(self.list_of_metrics('player_mean_task_reach_time'),axis=0)\n",
    "        self.player_sd_task_reach_time = np.nanstd(self.list_of_metrics('player_mean_task_reach_time'),axis=0)\n",
    "        self.player_mean_task_decision_time = np.nanmean(self.list_of_metrics('player_mean_task_decision_time'),axis=0)\n",
    "        self.player_median_task_decision_time = np.nanmedian(self.list_of_metrics('player_mean_task_decision_time'),axis=0)\n",
    "        self.player_sd_task_decision_time = np.nanstd(self.list_of_metrics('player_mean_task_decision_time'),axis = 0)\n",
    "        \n",
    "    def list_of_metrics(self,metric):\n",
    "        return [getattr(o,metric) for o in self.objects]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub1\n",
      "Sub2\n",
      "Sub3\n",
      "Sub4\n",
      "Sub5\n",
      "Sub6\n",
      "Sub7\n",
      "Sub8\n",
      "Sub9\n",
      "Sub10\n",
      "Sub11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:191: RuntimeWarning: invalid value encountered in true_divide\n",
      "  perc_reaction_wins = reaction_wins/total_reactions*100 # Array division\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:192: RuntimeWarning: invalid value encountered in true_divide\n",
      "  perc_reaction_incorrects = reaction_incorrects/total_reactions*100\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:193: RuntimeWarning: invalid value encountered in true_divide\n",
      "  perc_reaction_indecisions = reaction_indecisions/total_reactions*100\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:211: RuntimeWarning: Mean of empty slice\n",
      "  reaction_decision_time_means = np.nanmean(reaction_decision_time, axis = 1 )\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:213: RuntimeWarning: Mean of empty slice\n",
      "  agent_task_decision_time_reaction_means = np.nanmean(agent_task_decision_time_reactions, axis = 1)\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:202: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  perc_incorrects_that_were_gambles = gamble_indecisions/self.player_incorrects*100\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:206: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  perc_incorrects_that_were_reactions = reaction_indecisions/self.player_incorrects*100\n",
      "C:\\Users\\Seth Sullivan\\AppData\\Local\\Temp\\ipykernel_5292\\119843660.py:202: RuntimeWarning: invalid value encountered in true_divide\n",
      "  perc_incorrects_that_were_gambles = gamble_indecisions/self.player_incorrects*100\n"
     ]
    }
   ],
   "source": [
    "NUM_STDS_FOR_REACTION_TIME = 1\n",
    "\n",
    "subject_object_list = []\n",
    "for i in range(NUM_SUBJECTS):\n",
    "    subname = figures_pull_list[i]\n",
    "    print(subname)\n",
    "    subject_object_list.append(Subject(num_trials = 80, num_blocks = 6, num_control_trials = 50, num_washout_trials = 25, reaction_time = player_reaction_time[i], reaction_movement_time = player_reaction_movement_time[i],\n",
    "                                interval_trial_start = interval_trial_start[i], interval_reach_time = interval_reach_time[i], coincidence_trial_start = coincidence_trial_start[i], coincidence_reach_time = coincidence_reach_time[i],\n",
    "                                player_task_reach_time = player_task_reach_time[i], player_task_decision_time = player_task_decision_time[i], player_task_decision_array = player_task_decision_array[i],\n",
    "                                player_task_movement_time = player_task_movement_time[i], agent_task_reach_time = agent_task_reach_time[i], agent_task_decision_time = agent_task_decision_time[i], agent_task_decision_array = agent_task_decision_array[i],\n",
    "                                player_washout_reach_time = player_washout_reach_time[i], player_washout_decision_time = player_washout_decision_time[i], player_washout_decision_array = player_washout_decision_array[i],\n",
    "                                player_washout_movement_time = player_washout_movement_time[i], agent_washout_reach_time = agent_washout_reach_time[i], agent_washout_decision_time = agent_washout_decision_time[i], agent_washout_decision_array = agent_washout_decision_array[i],\n",
    "                                num_stds_for_reaction_time = NUM_STDS_FOR_REACTION_TIME\n",
    "                                ))\n",
    "    pickle.dump(subject_object_list[i], open(data_path + f'{subname}_OBJECT.pkl', 'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47.36842105, 46.15384615, 50.98039216, 45.45454545, 43.03797468,\n",
       "       37.31343284])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = subject_object_list[0]\n",
    "s.perc_gamble_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([47.36842105, 46.15384615, 50.98039216, 45.45454545, 43.03797468,\n",
       "       37.31343284])"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.num_stds_for_reaction_time = 2\n",
    "s.reaction_gamble_calculations(3)\n",
    "s.perc_gamble_wins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.num_stds_for_reaction_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.60722741892806"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group = Group(subject_object_list)\n",
    "group.coincidence_reach_time_sd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a00c1525417994d84940f6a64b96d4df953e4f0863c4f32c2c802abdf8195ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
