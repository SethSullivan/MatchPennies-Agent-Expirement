{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import dill\n",
    "from tqdm import tqdm \n",
    "from scipy.signal import find_peaks\n",
    "import sys\n",
    "import pyarrow as pa\n",
    "import io\n",
    "\n",
    "'''\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0,'D:\\OneDrive - University of Delaware - o365\\Desktop\\MatchPennies-Agent-Expirement')\n",
    "import filter_functions as ff\n",
    "from matplotlib.pyplot import cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fields pull and pull list\n",
    "os.chdir('D:\\OneDrive - University of Delaware - o365\\Subject_Data\\MatchPennies_Agent_Exp2')\n",
    "PATH = os.getcwd()\n",
    "\n",
    "analysis_pull_list = []\n",
    "with open(\"Analysis_Pull_List.txt\", \"r\") as file:\n",
    "    analysis_pull_list = file.read().splitlines()\n",
    "analysis_pull_list_reaction = [s+'_Reaction' for s in analysis_pull_list]\n",
    "analysis_pull_list_timing = [s+'_Timing' for s in analysis_pull_list]\n",
    "analysis_pull_list_task = [s+'_Task' for s in analysis_pull_list]\n",
    "\n",
    "with open(\"Fields_Pull.txt\", \"r\") as fields_pull:\n",
    "    fields_pull = fields_pull.read().splitlines()\n",
    "    \n",
    "num_subjects = len(analysis_pull_list)\n",
    "task_name = 'Matchpennies_Agent_Exp2'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Target information\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Row 0 = Start Target\n",
    "\n",
    "- Row 1 = Left screen, right target x pos (left target is 2\\*startx - right targetx)\n",
    "\n",
    "- Dim 1 = Radius\n",
    "\n",
    "- Dim 2 = Thickness of the circle edge (don't know if this matters)\n",
    "\n",
    "Dataframe is in centimeters, so need to divide everything by 100\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'Sub1_Reaction\\\\Sub1_ReactionTarget_Table.csv'\n",
    "df = pd.read_csv(file)\n",
    "df[\"X\"] = df[\"X\"]/100\n",
    "df[\"Y\"] = df[\"Y\"]/100\n",
    "df['Dim 1'] = df['Dim 1']/100 # Target table is in centimeters, I guess this doesn't matter but it makes me feel better\n",
    "df['Dim 2'] = df['Dim 2']/100\n",
    "# Target information for Right Hand (keeping this because the positions of target 3 and 4 are based on target 1 and start 1)\n",
    "startx = df.loc[0]['X']\n",
    "starty = df.loc[0]['Y']\n",
    "start_radius = df.loc[0]['Dim 1'] \n",
    "adjusted_start_radius = start_radius*1.0 # ! This determines what size target I use to determine target leave time \n",
    "\n",
    "target1x = df.loc[1]['X']\n",
    "target1y = df.loc[1]['Y']\n",
    "target1_radius = df.loc[1]['Dim 1']\n",
    "\n",
    "target2x = 2*startx - target1x\n",
    "target2y = target1y\n",
    "target2_radius = target1_radius\n",
    "\n",
    "# Timing target\n",
    "timing_targetx = startx\n",
    "timing_targety = target1y\n",
    "timing_target_pos = np.sqrt(timing_targetx**2 + timing_targety**2)\n",
    "timing_target_radius = target1_radius   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Reaction Task Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_force_reaction_time_single_trial(data):\n",
    "    max_force = np.nanmax(data)\n",
    "    index_max_force = np.nanargmax(data)\n",
    "    max25 = 0.25*max_force\n",
    "    max75 = 0.75*max_force\n",
    "    max25_timepoint,_ = min(enumerate(data[:index_max_force]), key=lambda x: abs(x[1]-max25)) # Enumerate force to get timepoint and value, only slice up to the max value, then find where it's closest to 25percent of max value\n",
    "    max75_timepoint,_ = min(enumerate(data[:index_max_force]), key=lambda x: abs(x[1]-max75))\n",
    "    x1vals = max25_timepoint\n",
    "    x2vals = max75_timepoint\n",
    "    y1vals = max25\n",
    "    y2vals = max75\n",
    "    if x1vals - x2vals == 0:\n",
    "        return np.nan\n",
    "    slopes = (y2vals - y1vals)/(x2vals - x1vals)\n",
    "    intercepts = y2vals - slopes*x2vals\n",
    "    time_at_zero = -intercepts/slopes\n",
    "    player_reaction_time_force = time_at_zero\n",
    "    return player_reaction_time_force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sub1_Reaction\n"
     ]
    },
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 37\u001b[0m\n\u001b[0;32m     35\u001b[0m filename \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00msubname\u001b[39m}\u001b[39;00m\u001b[39m\\\\\u001b[39;00m\u001b[39m{\u001b[39;00mtask_name\u001b[39m}\u001b[39;00m\u001b[39m_\u001b[39m\u001b[39m{\u001b[39;00msubname\u001b[39m}\u001b[39;00m\u001b[39m_C\u001b[39m\u001b[39m{\u001b[39;00mblock_number\u001b[39m}\u001b[39;00m\u001b[39m_TP\u001b[39m\u001b[39m{\u001b[39;00mtp_num\u001b[39m}\u001b[39;00m\u001b[39m_T\u001b[39m\u001b[39m{\u001b[39;00mblock_trial_num\u001b[39m}\u001b[39;00m\u001b[39m.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     36\u001b[0m filenames[i,j,k] \u001b[39m=\u001b[39m filename \u001b[39m# Store the file name\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m reaction_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(filename,engine\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m,dtype\u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mEvent_Codes\u001b[39;49m\u001b[39m'\u001b[39;49m: \u001b[39m'\u001b[39;49m\u001b[39mobject\u001b[39;49m\u001b[39m'\u001b[39;49m})  \u001b[39m# Read in data\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[39m#* Find the trial start time with event codes\u001b[39;00m\n\u001b[0;32m     39\u001b[0m event_code_col \u001b[39m=\u001b[39m reaction_df[\u001b[39m'\u001b[39m\u001b[39mEvent_Codes\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m         nrows\n\u001b[0;32m   1780\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[1;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Seth Sullivan\\anaconda3\\envs\\py311\\Lib\\site-packages\\pandas\\_libs\\parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: Calling read(nbytes) on source failed. Try engine='python'."
     ]
    }
   ],
   "source": [
    "#%% Get reaction time data\n",
    "block_table = pd.read_csv(f'Sub1_Reaction\\\\Sub1_ReactionBlock_Table.csv')\n",
    "num_trials = block_table['List_Reps'].loc[0]\n",
    "num_blocks = sum(~block_table['TP_LIST'].isna())\n",
    "trial_time = 8000\n",
    "tot_trials = int(sum(block_table['List_Reps']))\n",
    "\n",
    "filenames = np.empty((num_subjects, num_blocks,num_trials),dtype = object)\n",
    "reaction_trial_start = np.zeros((num_subjects, num_blocks,num_trials))*np.nan\n",
    "agent_reaction_leave_time = np.zeros((num_subjects, num_blocks,num_trials))*np.nan\n",
    "agent_reaction_decision_array = np.empty((num_subjects, num_blocks,num_trials))*np.nan\n",
    "\n",
    "reaction_trial_type_array = np.zeros((num_subjects, num_blocks,num_trials))*np.nan\n",
    "\n",
    "reaction_xypos_data = np.zeros((num_subjects, num_blocks,num_trials, trial_time,2))*np.nan\n",
    "reaction_dist_data = np.zeros((num_subjects, num_blocks,num_trials, trial_time))*np.nan\n",
    "reaction_xyvelocity_data = np.zeros((num_subjects, num_blocks,num_trials, trial_time,2))*np.nan\n",
    "reaction_speed_data = np.zeros((num_subjects, num_blocks,num_trials, trial_time))*np.nan\n",
    "reaction_xyforce_data = np.zeros((num_subjects, num_blocks,num_trials, trial_time,2))*np.nan\n",
    "reaction_force_data = np.zeros((num_subjects, num_blocks,num_trials, trial_time))*np.nan\n",
    "\n",
    "\n",
    "###-------------------------------------------------------------------------------------------------------\n",
    "for i in range(num_subjects):\n",
    "    subname = analysis_pull_list_reaction[i]\n",
    "    print(i, subname)\n",
    "    trial_table = pd.read_csv(subname+f'\\\\{subname}Trial_Table.csv')\n",
    "    for x in range(tot_trials):\n",
    "        block_trial_num = trial_table.iloc[x]['Block_Step']\n",
    "        block_number = trial_table.iloc[x]['Block_Row']\n",
    "        tp_num = trial_table.iloc[x]['TP_Row']\n",
    "        j = tp_num - 1 # Block number\n",
    "        k = block_trial_num - 1 # Trial number in that block\n",
    "        \n",
    "        filename = f\"{subname}\\\\{task_name}_{subname}_C{block_number}_TP{tp_num}_T{block_trial_num}.csv\"\n",
    "        filenames[i,j,k] = filename # Store the file name\n",
    "        reaction_df = pd.read_csv(filename,engine='c',dtype={'Event_Codes': 'object'})  # Read in data\n",
    "        #* Find the trial start time with event codes\n",
    "        event_code_col = reaction_df['Event_Codes']\n",
    "        if 'E_GO_TRIAL' in event_code_col.unique():\n",
    "            start_time = int(reaction_df[reaction_df['Event_Codes']=='E_GO_TRIAL'].index[-1]) #! Indexed at -1 bc if they need to redo the trial, we should use the last E_GO_TRIAL, not the first \n",
    "            reaction_trial_type_array[i,j,k] = 1 \n",
    "        elif 'E_SHUTOFF_TRIAL' in event_code_col.unique():\n",
    "            start_time = int(reaction_df[reaction_df['Event_Codes']=='E_SHUTOFF_TRIAL'].index[-1])\n",
    "            reaction_trial_type_array[i,j,k] = 0\n",
    "        else:\n",
    "            raise Exception('ERROR, event code not found')\n",
    "            \n",
    "        #* Get Reaction Time Data\n",
    "        end_time = start_time + trial_time      # Find how long the trial is, constant in this case to give people time to make it\n",
    "        reaction_trial_start[i,j,k] = start_time         # Store the start times, NOT THE AGENT DECISION TIMES\n",
    "        agent_reaction_leave_time[i,j,k] = reaction_df.iloc[start_time]['Agent_Decision_Time'] # Store the agent go time\n",
    "        agent_reaction_decision_array[i,j,k] = reaction_df.iloc[start_time+1]['Agent_Target_Selection']\n",
    "\n",
    "        cutoff_data = reaction_df.iloc[start_time:end_time] # Constrain data to the time that the trial starts, to the time that it ends\n",
    "            \n",
    "        reaction_xypos_data[i,j,k,:len(cutoff_data),:] = np.array(cutoff_data.drop(['Event_Codes'],axis=1)[['Left_HandX','Left_HandY']]) # Store left and right, slicing 3rd axis so if the trial ends early, then we just have nans at the end\n",
    "        lhx = cutoff_data['Left_HandX'].to_numpy() # Left hand x position\n",
    "        lhy = cutoff_data['Left_HandY'].to_numpy() # Left hand y position \n",
    "        lhx_vel = cutoff_data['Left_HandXVel'].to_numpy() # Left hand x velocity\n",
    "        lhy_vel = cutoff_data['Left_HandYVel'].to_numpy() # Left hand y velocity\n",
    "        lhx_force = cutoff_data['Left_FS_ForceX'].to_numpy()\n",
    "        lhy_force = cutoff_data['Left_FS_ForceY'].to_numpy()\n",
    "        \n",
    "        # Filter force\n",
    "        fx_nan_mask = ~np.isnan(lhx_force)\n",
    "        fy_nan_mask = ~np.isnan(lhy_force)\n",
    "        lhx_force_filt = np.zeros(trial_time)*np.nan\n",
    "        lhy_force_filt = np.zeros(trial_time)*np.nan\n",
    "        lhx_force_filt[:np.count_nonzero(fx_nan_mask)] = ff.Filter_KIN(lhx_force[fx_nan_mask])\n",
    "        lhy_force_filt[:np.count_nonzero(fy_nan_mask)] = ff.Filter_KIN(lhy_force[fy_nan_mask])\n",
    "        # Filter velocity\n",
    "        vx_nan_mask = ~np.isnan(lhx_vel)\n",
    "        vy_nan_mask = ~np.isnan(lhy_vel)\n",
    "        lhx_vel_filt = np.zeros(trial_time)*np.nan\n",
    "        lhy_vel_filt = np.zeros(trial_time)*np.nan\n",
    "        lhx_vel_filt[:np.count_nonzero(fx_nan_mask)] = ff.Filter_KIN(lhx_vel[fx_nan_mask])\n",
    "        lhy_vel_filt[:np.count_nonzero(fy_nan_mask)] = ff.Filter_KIN(lhy_vel[fy_nan_mask])\n",
    "        \n",
    "        #* Get force, speed, dist and store them in arrays\n",
    "        reaction_xypos_data[i,j,k,:len(lhx),0] = lhx\n",
    "        reaction_xypos_data[i,j,k,:len(lhx),1] = lhy\n",
    "        dist = np.sqrt((lhx-startx)**2 + (lhy-starty)**2) # Calculate dist\n",
    "        reaction_dist_data[i,j,k,:len(dist)] = dist # Store dist\n",
    "        \n",
    "        left_target_dist = np.sqrt(np.sqrt((lhx-target1x)**2 + (lhy-target1y)**2)) # Calculate dist\n",
    "        reaction_dist_data[i,j,k,:len(dist)] = dist # Store dist\n",
    "        dist = np.sqrt((lhx-startx)**2 + (lhy-starty)**2) # Calculate dist\n",
    "        reaction_dist_data[i,j,k,:len(dist)] = dist # Store dist\n",
    "        \n",
    "\n",
    "        reaction_xyforce_data[i,j,k,:len(lhx_force_filt),0] = lhx_force_filt\n",
    "        reaction_xyforce_data[i,j,k,:len(lhx_force_filt),1] = lhy_force_filt\n",
    "        force = np.sqrt(lhx_force_filt**2 + lhy_force_filt**2)\n",
    "        reaction_force_data[i,j,k,:] = force\n",
    "\n",
    "        reaction_xyvelocity_data[i,j,k,:len(lhx_vel_filt),0] = lhx_vel_filt\n",
    "        reaction_xyvelocity_data[i,j,k,:len(lhx_vel_filt),1] = lhy_vel_filt\n",
    "        speed = np.sqrt((lhx_vel_filt)**2 + (lhy_vel_filt)**2) # Calculate velocity\n",
    "        reaction_speed_data[i,j,k,:len(speed)] = speed # Store velocity \n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot all the traces\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Reaction Stuff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub1\n",
      "Subjects_Analyzed\\Sub1\\\n",
      "Sub2\n",
      "Subjects_Analyzed\\Sub2\\\n",
      "Sub4\n",
      "Subjects_Analyzed\\Sub4\\\n",
      "Sub5\n",
      "Subjects_Analyzed\\Sub5\\\n",
      "Sub6\n",
      "Subjects_Analyzed\\Sub6\\\n",
      "Sub7\n",
      "Subjects_Analyzed\\Sub7\\\n",
      "Sub8\n",
      "Subjects_Analyzed\\Sub8\\\n",
      "Sub9\n",
      "Subjects_Analyzed\\Sub9\\\n",
      "Sub10\n",
      "Subjects_Analyzed\\Sub10\\\n",
      "Sub11\n",
      "Subjects_Analyzed\\Sub11\\\n",
      "Sub12\n",
      "Subjects_Analyzed\\Sub12\\\n"
     ]
    }
   ],
   "source": [
    "i=-1\n",
    "for subname in analysis_pull_list:\n",
    "    i+=1\n",
    "    data_path = f'Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    print(subname)\n",
    "    print(data_path)\n",
    "    dill.dump(filenames[i,:,:], open(data_path + f'{subname}_filenames.pkl','wb'))                     \n",
    "    dill.dump(reaction_trial_start[i,:,:], open(data_path + f'{subname}_reaction_trial_start.pkl','wb'))                   \n",
    "    dill.dump(reaction_trial_type_array[i,:,:], open(data_path + f'{subname}_reaction_trial_type_array.pkl','wb'))                   \n",
    "    dill.dump(agent_reaction_leave_time[i,:,:], open(data_path + f'{subname}_agent_reaction_leave_time.pkl','wb'))  \n",
    "    dill.dump(agent_reaction_decision_array[i,:,:], open(data_path + f'{subname}_agent_reaction_decision_array.pkl','wb')) \n",
    "    dill.dump(reaction_trial_type_array[i,:,:], open(data_path + f'{subname}_trial_type_array.pkl','wb'))              \n",
    "    dill.dump(reaction_xypos_data[i,:,:], open(data_path + f'{subname}_reaction_xypos_data.pkl','wb'))           \n",
    "    dill.dump(reaction_dist_data[i,:,:], open(data_path + f'{subname}_reaction_dist_data.pkl','wb'))            \n",
    "    dill.dump(reaction_xyvelocity_data[i,:,:], open(data_path + f'{subname}_reaction_xyvelocity_data.pkl','wb'))      \n",
    "    dill.dump(reaction_speed_data[i,:,:], open(data_path + f'{subname}_reaction_speed_data.pkl','wb'))           \n",
    "    dill.dump(reaction_xyforce_data[i,:,:], open(data_path + f'{subname}_reaction_xyforce_data.pkl','wb'))         \n",
    "    dill.dump(reaction_force_data[i,:,:], open(data_path + f'{subname}_reaction_force_data.pkl','wb'))           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=-1\n",
    "# for subname in analysis_pull_list:\n",
    "#     i+=1\n",
    "#     data_path = f'Subjects_Analyzed\\\\{subname}\\\\'\n",
    "#     if not os.path.exists(data_path):\n",
    "#         os.makedirs(data_path)\n",
    "#     print(subname)\n",
    "#     print(data_path)\n",
    "#     dill.dump(reaction_dist_data[i,:,:], open(data_path + f'{subname}_reaction_dist_data.pkl', 'wb'))\n",
    "#     dill.dump(reaction_yforce_data[i,:,:], open(data_path + f'{subname}_reaction_yforce_data.pkl', 'wb'))\n",
    "#     dill.dump(player_reaction_time[i,:,:], open(data_path + f'{subname}_player_reaction_time.pkl', 'wb'))\n",
    "#     dill.dump(player_yforce_reaction_time[i,:,:], open(data_path + f'{subname}_player_yforce_reaction_time.pkl', 'wb'))\n",
    "#     dill.dump(player_reaction_movement_time[i,:,:], open(data_path + f'{subname}_player_reaction_movement_time.pkl', 'wb'))\n",
    "#     dill.dump(player_yforce_reaction_movement_time[i,:,:],open(data_path + f'{subname}_player_yforce_reaction_movement_time.pkl', 'wb'))\n",
    "#     dill.dump(player_reaction_plus_movement_time[i,:,:], open(data_path + f'{subname}_player_reaction_plus_movement_time.pkl', 'wb'))\n",
    "#     dill.dump(player_reaction_decision_array[i,:,:], open(data_path + f'{subname}_player_reaction_decision_array.pkl', 'wb'))\n",
    "#     dill.dump(agent_reaction_decision_array[i,:,:], open(data_path + f'{subname}_agent_reaction_decision_array.pkl', 'wb'))\n",
    "#     dill.dump(agent_reaction_leave_time[i,:,:], open(data_path + f'{subname}_agent_reaction_leave_time.pkl', 'wb'))\n",
    "#     dill.dump(trial_type_array[i,:,:], open(data_path + f'{subname}_reaction_trial_type_array.pkl', 'wb'))\n",
    "#     # dill.dump(agent_decision_time[i,:], open(data_path + f'{subname}_agent_decision_time.pkl', 'wb'))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Timing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sub1_Timing\n",
      "1 Sub2_Timing\n",
      "2 Sub4_Timing\n",
      "3 Sub5_Timing\n",
      "4 Sub6_Timing\n",
      "5 Sub7_Timing\n",
      "6 Sub8_Timing\n",
      "7 Sub9_Timing\n",
      "8 Sub10_Timing\n",
      "9 Sub11_Timing\n",
      "10 Sub12_Timing\n"
     ]
    }
   ],
   "source": [
    "coincidence_trial_time = 2500\n",
    "coincidence_trials = 50\n",
    "coincidence_trial_start = np.zeros((num_subjects, coincidence_trials))*np.nan\n",
    "coincidence_reach_time = np.zeros((num_subjects, coincidence_trials))*np.nan\n",
    "\n",
    "interval_trial_time = 2500\n",
    "interval_trials = 50\n",
    "interval_trial_start = np.zeros((num_subjects, interval_trials))*np.nan\n",
    "interval_reach_time =  np.zeros((num_subjects, interval_trials))*np.nan\n",
    "\n",
    "control_trials = interval_trials + coincidence_trials\n",
    "###-------------------------------------------------------------------------------------------------------\n",
    "for i in range(num_subjects):\n",
    "    sub_name = analysis_pull_list_timing[i]\n",
    "    print(i, sub_name)\n",
    "    file = f'{sub_name}\\\\{sub_name}Trial_Table.csv'\n",
    "    control_trial_table = pd.read_csv(file)\n",
    "    block_number = 1\n",
    "    tp_num = 1\n",
    "    for x in ((range(control_trials))):\n",
    "        block_number = control_trial_table.iloc[x]['Block_Row']\n",
    "        tp_num = control_trial_table.iloc[x]['TP_Row']\n",
    "        block_trial_num = control_trial_table.iloc[x]['Block_Step']\n",
    "        filename = f'{sub_name}\\\\{task_name}_{sub_name}_C{block_number}_TP{tp_num}_T{block_trial_num}.csv'\n",
    "        j = tp_num - 1 # Block number\n",
    "        k = block_trial_num - 1 # Trial number in that block\n",
    "        data = pd.read_csv(filename, low_memory=False)\n",
    "        # if 'Agent_Initial_Time' in data.columns:\n",
    "        #     data = data.rename(columns = {'Agent_Inital_Time':'Agent_Decision_Time'}, inplace=True)\n",
    "        # f = data['Agent_Decision_Time']\n",
    "        start_time = int(data[data['Event_Codes']=='E_SOUND_SIGNAL'].index[0])\n",
    "        # Get Interval Timing Data\n",
    "        if tp_num == 1:\n",
    "            end_time = start_time + interval_trial_time\n",
    "            interval_trial_start[i,k] = start_time # Store start time\n",
    "            lhx = np.array(data.iloc[start_time:end_time]['Left_HandX'])\n",
    "            lhy = np.array(data.iloc[start_time:end_time]['Left_HandY'])\n",
    "            q = np.argwhere(np.sqrt((lhx-timing_targetx)**2 + (lhy-timing_targety)**2) < timing_target_radius) # THIS NEEDS TO BE THE CENTER TARGET... x poisition should be the start\n",
    "            if np.size(q)>0:\n",
    "                interval_reach_time[i,k] = q[0]\n",
    "        # Get Coincidence Timing Data\n",
    "        if tp_num == 2:\n",
    "            end_time = start_time + coincidence_trial_time\n",
    "            coincidence_trial_start[i,k] = start_time # Store start time\n",
    "            lhx = np.array(data.iloc[start_time:end_time]['Left_HandX'])\n",
    "            lhy = np.array(data.iloc[start_time:end_time]['Left_HandY'])\n",
    "            q = np.argwhere(np.sqrt((lhx-timing_targetx)**2 + (lhy-timing_targety)**2) < timing_target_radius) # THIS NEEDS TO BE THE CENTER TARGET... x poisition should be the start\n",
    "            if np.size(q)>0:\n",
    "                coincidence_reach_time[i,k] = q[0] \n",
    "                \n",
    "            # #plot path for Reactions\n",
    "            if False:\n",
    "                for i in range(1):\n",
    "                    player_reaction_time[0,0] = 500\n",
    "                    plot_end_time = int(start_time + player_reaction_time[i,k])\n",
    "                    # plot_end_time = int(start_time + 500)\n",
    "                    lhx_new = np.array(data.iloc[start_time:int(plot_end_time)]['Left_HandX'])\n",
    "                    lhy_new = np.array(data.iloc[start_time:int(plot_end_time)]['Left_HandY'])\n",
    "                    plt.figure(dpi=300)\n",
    "                    circleR = plt.Circle((target1x,target1y), target1_radius, color = 'r', fill = False)\n",
    "                    circleL = plt.Circle((target2x,target2y), target2_radius, color = 'r', fill = False)\n",
    "                    startCirc = plt.Circle((startx,starty), start_radius, color = 'r', fill = False)\n",
    "                    fig, ax = plt.subplots()\n",
    "                    ax.add_patch(circleR)\n",
    "                    ax.add_patch(circleL)\n",
    "                    ax.add_patch(startCirc)\n",
    "                    plt.plot(lhx_new,lhy_new) \n",
    "                    # plt.scatter(lhx_new[int(s[0])], lhy_new[int(s[0])])\n",
    "                    print(player_reaction_time[i,k])\n",
    "                    # plt.title(\"w =%1.0f \" %i + \"x=%1.0f \"%j + \"c=%1.0f\"%k + 'vely=%1.5f'%vel_check)\n",
    "                    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Timing Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub1\n",
      "Subjects_Analyzed\\Sub1\\\n",
      "Sub2\n",
      "Subjects_Analyzed\\Sub2\\\n",
      "Sub4\n",
      "Subjects_Analyzed\\Sub4\\\n",
      "Sub5\n",
      "Subjects_Analyzed\\Sub5\\\n",
      "Sub6\n",
      "Subjects_Analyzed\\Sub6\\\n",
      "Sub7\n",
      "Subjects_Analyzed\\Sub7\\\n",
      "Sub8\n",
      "Subjects_Analyzed\\Sub8\\\n",
      "Sub9\n",
      "Subjects_Analyzed\\Sub9\\\n",
      "Sub10\n",
      "Subjects_Analyzed\\Sub10\\\n",
      "Sub11\n",
      "Subjects_Analyzed\\Sub11\\\n",
      "Sub12\n",
      "Subjects_Analyzed\\Sub12\\\n"
     ]
    }
   ],
   "source": [
    "i=-1\n",
    "for subname in analysis_pull_list:\n",
    "    i+=1\n",
    "    data_path = f'Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    print(subname)\n",
    "    print(data_path)\n",
    "    dill.dump(interval_trial_start[i,:], open(data_path + f'{subname}_interval_trial_start.pkl', 'wb'))\n",
    "    dill.dump(interval_reach_time[i,:], open(data_path + f'{subname}_interval_reach_time.pkl', 'wb'))\n",
    "    dill.dump(coincidence_trial_start[i,:], open(data_path + f'{subname}_coincidence_trial_start.pkl', 'wb'))\n",
    "    dill.dump(coincidence_reach_time[i,:], open(data_path + f'{subname}_coincidence_reach_time.pkl', 'wb'))\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Task Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Sub1_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:18<00:00, 17.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Sub2_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:10<00:00, 30.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Sub4_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:09<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Sub5_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:08<00:00, 37.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Sub6_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:08<00:00, 37.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Sub7_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:08<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Sub8_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:13<00:00, 23.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Sub9_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:10<00:00, 31.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Sub10_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:08<00:00, 38.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 Sub11_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:11<00:00, 26.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Sub12_Task\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 320/320 [00:08<00:00, 38.10it/s]\n"
     ]
    }
   ],
   "source": [
    "#%% Get task time task_df\n",
    "# Get trials, blocks, trial_time from trial_table\n",
    "path1 = PATH+'\\\\'+'Sub1_Task'\n",
    "task_df = pd.read_csv(path1+f'\\\\Sub1_TaskTrial_Table.csv')\n",
    "task_df = task_df.loc[task_df['Condition type']==3] # Only get the task condition \n",
    "num_trials = int(task_df.iloc[-1]['Block_Step']) # number of trials in each block\n",
    "num_blocks = int(task_df.iloc[-1]['Block_Row'])\n",
    "tot_trials = int(num_trials*num_blocks)\n",
    "trial_time = int(task_df.iloc[0]['Condition time']) + 500\n",
    "task_df_columns = len(fields_pull)\n",
    "# Need to be kept outside subject for-loop\n",
    "task_trial_start          = np.zeros((num_subjects, num_blocks, num_trials))*np.nan\n",
    "task_filenames            = np.empty((num_subjects, num_blocks,num_trials),dtype = object)\n",
    "agent_task_leave_time     = np.zeros((num_subjects, num_blocks,num_trials))*np.nan\n",
    "agent_task_decision_array = np.empty((num_subjects, num_blocks,num_trials))*np.nan\n",
    "task_xypos_data           = np.zeros((num_subjects, num_blocks,num_trials, trial_time,2))*np.nan\n",
    "task_dist_data            = np.zeros((num_subjects, num_blocks,num_trials, trial_time))*np.nan\n",
    "task_xyvelocity_data      = np.zeros((num_subjects, num_blocks,num_trials, trial_time,2))*np.nan\n",
    "task_speed_data           = np.zeros((num_subjects, num_blocks,num_trials, trial_time))*np.nan\n",
    "task_xyforce_data         = np.zeros((num_subjects, num_blocks,num_trials, trial_time,2))*np.nan\n",
    "task_force_data           = np.zeros((num_subjects, num_blocks,num_trials, trial_time))*np.nan\n",
    "\n",
    "###-------------------------------------------------------------------------------------------------------\n",
    "for i in range(num_subjects):\n",
    "    sub_name = analysis_pull_list_task[i]\n",
    "    print(i, sub_name)\n",
    "    path1 = PATH+'\\\\'+sub_name\n",
    "    file = path1+f'\\\\{sub_name}Trial_Table.csv'\n",
    "    trial_table = pd.read_csv(file)\n",
    "    # Splt trial table into task and washout based on condition type\n",
    "    if sub_name == 'Sub12_Task':\n",
    "        task_trial_table = trial_table[trial_table['Condition_type']==3]\n",
    "    else:\n",
    "        task_trial_table = trial_table[trial_table['Condition type']==3]\n",
    "    \n",
    "    block_number = 1\n",
    "    tp_num = 1\n",
    "    for x in tqdm((range(tot_trials))):\n",
    "        block_number = task_trial_table.iloc[x]['Block_Row']\n",
    "        tp_num = task_trial_table.iloc[x]['TP_Row']\n",
    "        block_trial_num = task_trial_table.iloc[x]['Block_Step']\n",
    "        j = tp_num - task_trial_table['TP_Row'].min()  # Block number\n",
    "        k = block_trial_num - 1 # Trial number in that block\n",
    "        \n",
    "        filename = PATH+f\"\\\\{sub_name}\\\\{task_name}_{sub_name}_C{block_number}_TP{tp_num}_T{block_trial_num}.csv\"\n",
    "        task_filenames[i,j,k] = filename\n",
    "        task_df = pd.read_csv(filename, low_memory=False)\n",
    "        start_time = int(task_df[task_df['Event_Codes']=='E_SOUND_SIGNAL'].index[0])\n",
    "        task_trial_start[i,j,k] = start_time\n",
    "        end_time = start_time + trial_time\n",
    "        task_df = task_df.drop(columns ='Event_Codes') # Drop event codes cuz it's not number column and can't be an array \n",
    "        cutoff_data = task_df.iloc[start_time:end_time]\n",
    "        agent_task_decision_array[i,j,k] = task_df.iloc[start_time+1]['Agent_Target_Selection']\n",
    "        agent_task_leave_time[i,j,k] = task_df.iloc[start_time]['Agent_Decision_Time'] # Store the agent go time\n",
    "\n",
    "        \n",
    "        lhx = cutoff_data['Left_HandX'].to_numpy() # Left hand x position\n",
    "        lhy = cutoff_data['Left_HandY'].to_numpy() # Left hand y position \n",
    "        lhx_vel = cutoff_data['Left_HandXVel'].to_numpy() # Left hand x velocity\n",
    "        lhy_vel = cutoff_data['Left_HandYVel'].to_numpy() # Left hand y velocity\n",
    "        lhx_force = cutoff_data['Left_FS_ForceX'].to_numpy()\n",
    "        lhy_force = cutoff_data['Left_FS_ForceY'].to_numpy()\n",
    "        \n",
    "        # Filter force\n",
    "        fx_nan_mask = ~np.isnan(lhx_force)\n",
    "        fy_nan_mask = ~np.isnan(lhy_force)\n",
    "        lhx_force_filt = np.zeros(trial_time)*np.nan\n",
    "        lhy_force_filt = np.zeros(trial_time)*np.nan\n",
    "        lhx_force_filt[:np.count_nonzero(fx_nan_mask)] = ff.Filter_KIN(lhx_force[fx_nan_mask])\n",
    "        lhy_force_filt[:np.count_nonzero(fy_nan_mask)] = ff.Filter_KIN(lhy_force[fy_nan_mask])\n",
    "        # Filter velocity\n",
    "        vx_nan_mask = ~np.isnan(lhx_vel)\n",
    "        vy_nan_mask = ~np.isnan(lhy_vel)\n",
    "        lhx_vel_filt = np.zeros(trial_time)*np.nan\n",
    "        lhy_vel_filt = np.zeros(trial_time)*np.nan\n",
    "        lhx_vel_filt[:np.count_nonzero(fx_nan_mask)] = ff.Filter_KIN(lhx_vel[fx_nan_mask])\n",
    "        lhy_vel_filt[:np.count_nonzero(fy_nan_mask)] = ff.Filter_KIN(lhy_vel[fy_nan_mask])\n",
    "        \n",
    "        #* Get force, speed, dist and store them in arrays\n",
    "        task_xypos_data[i,j,k,:len(lhx),0] = lhx\n",
    "        task_xypos_data[i,j,k,:len(lhx),1] = lhy\n",
    "        dist = np.sqrt((lhx-startx)**2 + (lhy-starty)**2) # Calculate dist\n",
    "        task_dist_data[i,j,k,:len(dist)] = dist # Store dist\n",
    "        \n",
    "        left_target_dist = np.sqrt(np.sqrt((lhx-target1x)**2 + (lhy-target1y)**2)) # Calculate dist\n",
    "        task_dist_data[i,j,k,:len(dist)] = dist # Store dist\n",
    "        dist = np.sqrt((lhx-startx)**2 + (lhy-starty)**2) # Calculate dist\n",
    "        task_dist_data[i,j,k,:len(dist)] = dist # Store dist\n",
    "        \n",
    "\n",
    "        task_xyforce_data[i,j,k,:len(lhx_force_filt),0] = lhx_force_filt\n",
    "        task_xyforce_data[i,j,k,:len(lhx_force_filt),1] = lhy_force_filt\n",
    "        force = np.sqrt(lhx_force_filt**2 + lhy_force_filt**2)\n",
    "        task_force_data[i,j,k,:] = force\n",
    "\n",
    "        task_xyvelocity_data[i,j,k,:len(lhx_vel_filt),0] = lhx_vel_filt\n",
    "        task_xyvelocity_data[i,j,k,:len(lhx_vel_filt),1] = lhy_vel_filt\n",
    "        speed = np.sqrt((lhx_vel_filt)**2 + (lhy_vel_filt)**2) # Calculate velocity\n",
    "        task_speed_data[i,j,k,:len(speed)] = speed # Store velocity \n",
    "           "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickle Task Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub1\n",
      "Sub2\n",
      "Sub4\n",
      "Sub5\n",
      "Sub6\n",
      "Sub7\n",
      "Sub8\n",
      "Sub9\n",
      "Sub10\n",
      "Sub11\n",
      "Sub12\n"
     ]
    }
   ],
   "source": [
    "i=-1\n",
    "for subname in analysis_pull_list:\n",
    "    i+=1\n",
    "    data_path = PATH+f'\\\\Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "    print(subname)\n",
    "    dill.dump(task_trial_start[i,...], open(data_path + f'{subname}_task_trial_start.pkl','wb'))          \n",
    "    dill.dump(task_filenames[i,...], open(data_path + f'{subname}_task_filenames.pkl','wb'))            \n",
    "    dill.dump(agent_task_leave_time[i,...], open(data_path + f'{subname}_agent_task_leave_time.pkl','wb'))  \n",
    "    dill.dump(agent_task_decision_array[i,...], open(data_path + f'{subname}_agent_task_decision_array.pkl','wb')) \n",
    "    dill.dump(task_xypos_data[i,...], open(data_path + f'{subname}_task_xypos_data.pkl','wb'))           \n",
    "    dill.dump(task_dist_data[i,...], open(data_path + f'{subname}_task_dist_data.pkl','wb'))            \n",
    "    dill.dump(task_xyvelocity_data[i,...], open(data_path + f'{subname}_task_xyvelocity_data.pkl','wb'))      \n",
    "    dill.dump(task_speed_data[i,...], open(data_path + f'{subname}_task_speed_data.pkl','wb'))           \n",
    "    dill.dump(task_xyforce_data[i,...], open(data_path + f'{subname}_task_xyforce_data.pkl','wb'))         \n",
    "    dill.dump(task_force_data[i,...], open(data_path + f'{subname}_task_force_data.pkl','wb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a00c1525417994d84940f6a64b96d4df953e4f0863c4f32c2c802abdf8195ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
