{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import dill\n",
    "import importlib\n",
    "import pandas as pd\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Subject_Object' from 'D:\\\\OneDrive - University of Delaware - o365\\\\Desktop\\\\MatchPennies-Agent-Expirement\\\\Subject_Object.py'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.insert(0,r'D:\\OneDrive - University of Delaware - o365\\Desktop\\MatchPennies-Agent-Expirement')\n",
    "import Subject_Object\n",
    "importlib.reload(Subject_Object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Thangs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('D:\\OneDrive - University of Delaware - o365\\Subject_Data\\MatchPennies_Agent_Exp1')\n",
    "PATH = os.getcwd()\n",
    "# Fields pull and pull list\n",
    "figures_pull_list = []\n",
    "fields_pull = []\n",
    "with open(PATH+\"\\\\Figures_Pull_List.txt\", \"r\") as pull_file:\n",
    "    figures_pull_list = pull_file.read().splitlines()\n",
    "with open(PATH+\"\\\\Fields_Pull.txt\", \"r\") as fields_pull:\n",
    "    fields_pull = fields_pull.read().splitlines()\n",
    "NUM_SUBJECTS = len(figures_pull_list)\n",
    "task_name = 'Seth_MatchPennies_Agent_Exp1'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## dill Load"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dill Load Control Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sub1\n",
      "Sub2\n",
      "Sub3\n",
      "Sub4\n",
      "Sub5\n",
      "Sub6\n",
      "Sub7\n",
      "Sub8\n",
      "Sub9\n",
      "Sub10\n",
      "Sub11\n",
      "Sub12\n",
      "Sub13\n",
      "Sub14\n",
      "Sub15\n",
      "Sub16\n",
      "Sub17\n",
      "Sub18\n",
      "Sub19\n",
      "Sub20\n"
     ]
    }
   ],
   "source": [
    "coincidence_trials = 50\n",
    "reaction_trials = 50\n",
    "interval_trials = 50\n",
    "# ---------------Controls-------------------------\n",
    "player_reaction_decision_array         = np.empty((NUM_SUBJECTS, reaction_trials))\n",
    "agent_reaction_decision_array          = np.empty((NUM_SUBJECTS, reaction_trials))\n",
    "player_reaction_time                   = np.zeros([NUM_SUBJECTS,reaction_trials])*np.nan \n",
    "player_reaction_movement_time          = np.zeros([NUM_SUBJECTS,reaction_trials])*np.nan\n",
    "player_reaction_plus_movement_time     = np.zeros([NUM_SUBJECTS,reaction_trials])*np.nan\n",
    "reaction_trial_start                   = np.zeros((NUM_SUBJECTS,reaction_trials))*np.nan \n",
    "coincidence_trial_start                = np.zeros((NUM_SUBJECTS, coincidence_trials))*np.nan\n",
    "coincidence_reach_time                 = np.zeros((NUM_SUBJECTS, coincidence_trials))*np.nan\n",
    "interval_trial_start                   = np.zeros((NUM_SUBJECTS, interval_trials))*np.nan\n",
    "interval_reach_time                    = np.zeros((NUM_SUBJECTS, interval_trials))*np.nan\n",
    "\n",
    "for i in range(NUM_SUBJECTS):\n",
    "    subname = figures_pull_list[i]\n",
    "    data_path = PATH+f'\\\\Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    reaction_trial_start[i,:]                = dill.load(open(data_path + f'{subname}_reaction_trial_start.pkl', 'rb'))\n",
    "    player_reaction_time[i,:]                = dill.load(open(data_path + f'{subname}_player_reaction_time.pkl', 'rb'))\n",
    "    player_reaction_movement_time[i,:]       = dill.load(open(data_path + f'{subname}_player_reaction_movement_time.pkl', 'rb'))\n",
    "    player_reaction_plus_movement_time[i,:]  = dill.load(open(data_path + f'{subname}_player_reaction_plus_movement_time.pkl', 'rb'))\n",
    "    player_reaction_decision_array[i,:]      = dill.load(open(data_path + f'{subname}_player_reaction_decision_array.pkl', 'rb'))\n",
    "    agent_reaction_decision_array[i,:]       = dill.load(open(data_path + f'{subname}_agent_reaction_decision_time.pkl', 'rb'))\n",
    "    reaction_trial_start[i,:]                = dill.load(open(data_path + f'{subname}_reaction_trial_start.pkl', 'rb'))\n",
    "    interval_trial_start[i,:]                = dill.load(open(data_path + f'{subname}_interval_trial_start.pkl', 'rb'))\n",
    "    interval_reach_time[i,:]                 = dill.load(open(data_path + f'{subname}_interval_reach_time.pkl', 'rb'))\n",
    "    coincidence_trial_start[i,:]             = dill.load(open(data_path + f'{subname}_coincidence_trial_start.pkl', 'rb'))\n",
    "    coincidence_reach_time[i,:]              = dill.load(open(data_path + f'{subname}_coincidence_reach_time.pkl', 'rb'))\n",
    "    print(subname)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dill Load Task and Washout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = PATH+'\\\\'+'Sub1_Task'\n",
    "task_df = pd.read_csv(path1+f'\\\\Sub1_TaskTrial_Table.csv')\n",
    "task_df = task_df.loc[task_df['Condition type']==3] # Only get the task condition \n",
    "num_trials = int(task_df.iloc[-1]['Block_Step']) # number of trials in each block\n",
    "num_blocks = int(task_df.iloc[-1]['Block_Row']/2)\n",
    "tot_trials = int(num_trials*num_blocks)\n",
    "trial_time = int(task_df.iloc[0]['Condition time'])\n",
    "task_df_columns = len(fields_pull)\n",
    "trial_table = np.empty((NUM_SUBJECTS, tot_trials, 4), int)\n",
    "#----------------- TASK--------------------------------\n",
    "player_task_reach_time       = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "player_task_decision_time    = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "player_task_decision_array   = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "player_task_movement_time    = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "agent_task_reach_time        = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "agent_task_decision_time     = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "agent_task_decision_array    = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "\n",
    "# -------------- Washout-------------------\n",
    "washout_trials = 25\n",
    "player_washout_reach_time      = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "player_washout_decision_time   = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "player_washout_decision_array  = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "player_washout_movement_time   = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "agent_washout_reach_time       = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "agent_washout_decision_time    = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "agent_washout_decision_array   = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "agent_washout_movement_time    = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "\n",
    "for i in range(NUM_SUBJECTS):\n",
    "    subname = figures_pull_list[i]\n",
    "    data_path = PATH+f'\\\\Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    #task_data[i,:,:,:,:] = dill.load(open(data_path + f'{subname}_task_data.pkl', 'rb'))\n",
    "    player_task_decision_time[i,:,:]      = dill.load( open(data_path + f'{subname}_player_task_decision_time.pkl', 'rb'))\n",
    "    player_task_decision_array[i,:,:]     = dill.load( open(data_path + f'{subname}_player_task_decision_array.pkl', 'rb'))\n",
    "    player_task_movement_time[i,:,:]      = dill.load(open(data_path + f'{subname}_player_task_movement_time.pkl', 'rb'))\n",
    "    player_task_reach_time[i,:,:]         = dill.load( open(data_path + f'{subname}_player_task_reach_time.pkl', 'rb'))\n",
    "    agent_task_decision_time[i,:,:]       = dill.load( open(data_path + f'{subname}_agent_task_decision_time.pkl', 'rb'))\n",
    "    agent_task_decision_array[i,:,:]      = dill.load(open(data_path + f'{subname}_agent_task_decision_array.pkl', 'rb'))\n",
    "    agent_task_reach_time[i,:,:]          = agent_task_decision_time[i,:,:] + 150\n",
    "    player_washout_decision_time[i,:,:]   = dill.load( open(data_path + f'{subname}_player_washout_decision_time.pkl', 'rb'))\n",
    "    player_washout_decision_array[i,:,:]  = dill.load( open(data_path + f'{subname}_player_washout_decision_array.pkl', 'rb'))\n",
    "    player_washout_movement_time[i,:,:]   = dill.load(open(data_path + f'{subname}_player_washout_movement_time.pkl', 'rb'))\n",
    "    player_washout_reach_time[i,:,:]      = dill.load( open(data_path + f'{subname}_player_washout_reach_time.pkl', 'rb'))\n",
    "    agent_washout_decision_time[i,:,:]    = dill.load( open(data_path + f'{subname}_agent_washout_decision_time.pkl', 'rb'))\n",
    "    agent_washout_decision_array[i,:,:]   = dill.load(open(data_path + f'{subname}_agent_washout_decision_array.pkl', 'rb'))\n",
    "    agent_washout_reach_time[i,:,:]       = dill.load(open(data_path + f'{subname}_agent_washout_reach_time.pkl', 'rb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subject Objects"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = PATH+'\\\\'+'Sub1_Task'\n",
    "task_df = pd.read_csv(path1+f'\\\\Sub1_TaskTrial_Table.csv')\n",
    "task_df = task_df.loc[task_df['Condition type']==3] # Only get the task condition \n",
    "num_trials = int(task_df.iloc[-1]['Block_Step']) # number of trials in each block\n",
    "num_blocks = int(task_df.iloc[-1]['Block_Row']/2)\n",
    "tot_trials = int(num_trials*num_blocks)\n",
    "trial_time = int(task_df.iloc[0]['Condition time'])\n",
    "task_df_columns = len(fields_pull)\n",
    "trial_table = np.empty((NUM_SUBJECTS, tot_trials, 4), int)\n",
    "#----------------- TASK--------------------------------\n",
    "player_task_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "player_task_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "player_task_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "player_task_movement_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "agent_task_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "agent_task_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan \n",
    "agent_task_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,num_trials])*np.nan\n",
    "\n",
    "# -------------- Washout-------------------\n",
    "washout_trials = 25\n",
    "player_washout_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "player_washout_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "player_washout_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "player_washout_movement_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "agent_washout_reach_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "agent_washout_decision_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan \n",
    "agent_washout_decision_array = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "agent_washout_movement_time = np.zeros([NUM_SUBJECTS,num_blocks,washout_trials])*np.nan\n",
    "\n",
    "for i in range(NUM_SUBJECTS):\n",
    "    subname = figures_pull_list[i]\n",
    "    data_path = PATH+f'\\\\Subjects_Analyzed\\\\{subname}\\\\'\n",
    "    #task_data[i,:,:,:,:] = dill.load(open(data_path + f'{subname}_task_data.pkl', 'rb'))\n",
    "    player_task_decision_time[i,:,:] = dill.load(open(data_path + f'{subname}_player_task_decision_time.pkl', 'rb'))\n",
    "    player_task_decision_array[i,:,:] = dill.load(open(data_path + f'{subname}_player_task_decision_array.pkl', 'rb'))\n",
    "    player_task_movement_time[i,:,:] = dill.load(open(data_path + f'{subname}_player_task_movement_time.pkl', 'rb'))\n",
    "    player_task_reach_time[i,:,:] = dill.load(open(data_path + f'{subname}_player_task_reach_time.pkl', 'rb'))\n",
    "    agent_task_decision_time[i,:,:] = dill.load(open(data_path + f'{subname}_agent_task_decision_time.pkl', 'rb'))\n",
    "    agent_task_decision_array[i,:,:] = dill.load(open(data_path + f'{subname}_agent_task_decision_array.pkl', 'rb'))\n",
    "    agent_task_reach_time[i,:,:] = agent_task_decision_time[i,:,:] + 150\n",
    "    player_washout_decision_time[i,:,:] = dill.load(open(data_path + f'{subname}_player_washout_decision_time.pkl', 'rb'))\n",
    "    player_washout_decision_array[i,:,:] = dill.load(open(data_path + f'{subname}_player_washout_decision_array.pkl', 'rb'))\n",
    "    player_washout_movement_time[i,:,:] = dill.load(open(data_path + f'{subname}_player_washout_movement_time.pkl', 'rb'))\n",
    "    player_washout_reach_time[i,:,:] = dill.load(open(data_path + f'{subname}_player_washout_reach_time.pkl', 'rb'))\n",
    "    agent_washout_decision_time[i,:,:] = dill.load(open(data_path + f'{subname}_agent_washout_decision_time.pkl', 'rb'))\n",
    "    agent_washout_decision_array[i,:,:] = dill.load(open(data_path + f'{subname}_agent_washout_decision_array.pkl', 'rb'))\n",
    "    agent_washout_reach_time[i,:,:] = dill.load(open(data_path + f'{subname}_agent_washout_reach_time.pkl', 'rb'))\n",
    "\n",
    "NUM_STDS = [0,1,2]\n",
    "append_dict = {0:'reaction_time_no_sd_off',1:'reaction_time_minus_sd',2:'reaction_time_minus_2sd'}\n",
    "data_path = 'Subjects_Analyzed\\\\'\n",
    "for sd in NUM_STDS:\n",
    "    subject_object_dict = {}\n",
    "    for i in range(NUM_SUBJECTS):\n",
    "        subname = figures_pull_list[i]\n",
    "        subject_object = Subject_Object.Subject(\n",
    "            experiment = 'Exp1',select_trials = 'All Trials', num_trials = 80, num_blocks = 6, num_control_trials = 50, num_washout_trials = 25, reaction_time = player_reaction_time[i], reaction_movement_time = player_reaction_movement_time[i],reaction_plus_movement_time = player_reaction_plus_movement_time[i],\n",
    "            interval_trial_start = interval_trial_start[i], interval_reach_time = interval_reach_time[i], coincidence_trial_start = coincidence_trial_start[i], coincidence_reach_time = coincidence_reach_time[i],\n",
    "            player_task_reach_time = player_task_reach_time[i], player_task_leave_time = player_task_decision_time[i], player_task_decision_array = player_task_decision_array[i],\n",
    "            player_task_movement_time = player_task_movement_time[i], agent_task_reach_time = agent_task_reach_time[i], agent_task_leave_time = agent_task_decision_time[i], agent_task_decision_array = agent_task_decision_array[i],\n",
    "            player_washout_reach_time = player_washout_reach_time[i], player_washout_leave_time = player_washout_decision_time[i], player_washout_decision_array = player_washout_decision_array[i],\n",
    "            player_washout_movement_time = player_washout_movement_time[i], agent_washout_reach_time = agent_washout_reach_time[i], agent_washout_leave_time = agent_washout_decision_time[i], agent_washout_decision_array = agent_washout_decision_array[i],\n",
    "            num_stds_for_reaction_time = sd\n",
    "            )\n",
    "        subject_object_dict.update({subname:subject_object})\n",
    "        \n",
    "        dill.dump(subject_object_dict[subname], open(data_path + f'{subname}\\\\{subname}_object_{append_dict[sd]}.pkl', 'wb'))\n",
    "    dill.dump(subject_object_dict,open(data_path + f'subject_object_{append_dict[sd]}_dict.pkl', 'wb'))\n",
    "\n",
    "    group = Subject_Object.Group(list(subject_object_dict.values()))\n",
    "    dill.dump(group,open(data_path + f'group_object_{append_dict[sd]}.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Half of Each Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STDS = [0,1,2]\n",
    "append_dict = {0:'reaction_time_no_sd_off',1:'reaction_time_minus_sd',2:'reaction_time_minus_2sd'}\n",
    "data_path = 'Subjects_Analyzed\\\\'\n",
    "for sd in NUM_STDS:\n",
    "    subject_object_dict = {}\n",
    "    for i in range(NUM_SUBJECTS):\n",
    "        subname = figures_pull_list[i]\n",
    "        subject_object = Subject_Object.Subject(\n",
    "            experiment = 'Exp1',select_trials = 'First Half', num_trials = 80, num_blocks = 6, num_control_trials = 50, num_washout_trials = 25, reaction_time = player_reaction_time[i], reaction_movement_time = player_reaction_movement_time[i],reaction_plus_movement_time = player_reaction_plus_movement_time[i],\n",
    "            interval_trial_start = interval_trial_start[i], interval_reach_time = interval_reach_time[i], coincidence_trial_start = coincidence_trial_start[i], coincidence_reach_time = coincidence_reach_time[i],\n",
    "            player_task_reach_time = player_task_reach_time[i], player_task_leave_time = player_task_decision_time[i], player_task_decision_array = player_task_decision_array[i],\n",
    "            player_task_movement_time = player_task_movement_time[i], agent_task_reach_time = agent_task_reach_time[i], agent_task_leave_time = agent_task_decision_time[i], agent_task_decision_array = agent_task_decision_array[i],\n",
    "            player_washout_reach_time = player_washout_reach_time[i], player_washout_leave_time = player_washout_decision_time[i], player_washout_decision_array = player_washout_decision_array[i],\n",
    "            player_washout_movement_time = player_washout_movement_time[i], agent_washout_reach_time = agent_washout_reach_time[i], agent_washout_leave_time = agent_washout_decision_time[i], agent_washout_decision_array = agent_washout_decision_array[i],\n",
    "            num_stds_for_reaction_time = sd\n",
    "            )\n",
    "        subject_object_dict.update({subname:subject_object})\n",
    "        \n",
    "        dill.dump(subject_object_dict[subname], open(data_path + f'{subname}\\\\{subname}_object_first_half_{append_dict[sd]}.pkl', 'wb'))\n",
    "    dill.dump(subject_object_dict,open(data_path + f'subject_object_first_half_{append_dict[sd]}_dict.pkl', 'wb'))\n",
    "\n",
    "    group = Subject_Object.Group(list(subject_object_dict.values()))\n",
    "    dill.dump(group,open(data_path + f'group_object_first_half_{append_dict[sd]}.pkl','wb'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second Half of Each Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_STDS = [0,1,2]\n",
    "append_dict = {0:'reaction_time_no_sd_off',1:'reaction_time_minus_sd',2:'reaction_time_minus_2sd'}\n",
    "data_path = 'Subjects_Analyzed\\\\'\n",
    "for sd in NUM_STDS:\n",
    "    subject_object_dict = {}\n",
    "    for i in range(NUM_SUBJECTS):\n",
    "        subname = figures_pull_list[i]\n",
    "        subject_object = Subject_Object.Subject(\n",
    "            experiment = 'Exp1',select_trials = 'Second Half', num_trials = 80, num_blocks = 6, num_control_trials = 50, num_washout_trials = 25, reaction_time = player_reaction_time[i], reaction_movement_time = player_reaction_movement_time[i],reaction_plus_movement_time = player_reaction_plus_movement_time[i],\n",
    "            interval_trial_start = interval_trial_start[i], interval_reach_time = interval_reach_time[i], coincidence_trial_start = coincidence_trial_start[i], coincidence_reach_time = coincidence_reach_time[i],\n",
    "            player_task_reach_time = player_task_reach_time[i], player_task_leave_time = player_task_decision_time[i], player_task_decision_array = player_task_decision_array[i],\n",
    "            player_task_movement_time = player_task_movement_time[i], agent_task_reach_time = agent_task_reach_time[i], agent_task_leave_time = agent_task_decision_time[i], agent_task_decision_array = agent_task_decision_array[i],\n",
    "            player_washout_reach_time = player_washout_reach_time[i], player_washout_leave_time = player_washout_decision_time[i], player_washout_decision_array = player_washout_decision_array[i],\n",
    "            player_washout_movement_time = player_washout_movement_time[i], agent_washout_reach_time = agent_washout_reach_time[i], agent_washout_leave_time = agent_washout_decision_time[i], agent_washout_decision_array = agent_washout_decision_array[i],\n",
    "            num_stds_for_reaction_time = sd\n",
    "            )\n",
    "        subject_object_dict.update({subname:subject_object})\n",
    "        \n",
    "        dill.dump(subject_object_dict[subname], open(data_path + f'{subname}\\\\{subname}_object_second_half_{append_dict[sd]}.pkl', 'wb'))\n",
    "    dill.dump(subject_object_dict,open(data_path + f'subject_object_second_half_{append_dict[sd]}_dict.pkl', 'wb'))\n",
    "\n",
    "    group = Subject_Object.Group(list(subject_object_dict.values()))\n",
    "    dill.dump(group,open(data_path + f'group_object_second_half_{append_dict[sd]}.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\OneDrive - University of Delaware - o365\\Desktop\\MatchPennies-Agent-Expirement\\Subject_Object.py:393: RuntimeWarning: Mean of empty slice\n",
      "  self.binned_player_task_leave_times_mean            = np.nanmean(self.binned_player_task_leave_times,axis=2) # Mean for each bin, each condition\n",
      "D:\\OneDrive - University of Delaware - o365\\Desktop\\MatchPennies-Agent-Expirement\\Subject_Object.py:394: RuntimeWarning: Mean of empty slice\n",
      "  self.binned_player_minus_agent_task_leave_time_mean = np.nanmean(self.binned_player_minus_agent_task_leave_time,axis=2)\n"
     ]
    }
   ],
   "source": [
    "group.analyze_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a00c1525417994d84940f6a64b96d4df953e4f0863c4f32c2c802abdf8195ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
